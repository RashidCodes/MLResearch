{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tree of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#INTRODUCTION\" data-toc-modified-id=\"INTRODUCTION-1\">INTRODUCTION</a></span></li><li><span><a href=\"#DEDUCTIONS-FROM-THE-PREVIOUS-RESEARCH.\" data-toc-modified-id=\"DEDUCTIONS-FROM-THE-PREVIOUS-RESEARCH.-2\">DEDUCTIONS FROM THE PREVIOUS RESEARCH.</a></span></li><li><span><a href=\"#ASSUMPTIONS\" data-toc-modified-id=\"ASSUMPTIONS-3\">ASSUMPTIONS</a></span></li><li><span><a href=\"#TACKLING-ASSUMPTIONS\" data-toc-modified-id=\"TACKLING-ASSUMPTIONS-4\">TACKLING ASSUMPTIONS</a></span><ul class=\"toc-item\"><li><span><a href=\"#Investigating-Relationships-between-features\" data-toc-modified-id=\"Investigating-Relationships-between-features-4.1\">Investigating Relationships between features</a></span></li><li><span><a href=\"#Checking-Collinearity\" data-toc-modified-id=\"Checking-Collinearity-4.2\">Checking Collinearity</a></span></li><li><span><a href=\"#Checking-for-Class-Imbalance\" data-toc-modified-id=\"Checking-for-Class-Imbalance-4.3\">Checking for Class Imbalance</a></span></li><li><span><a href=\"#DEALING-WITH-CLASS-IMBALANCE\" data-toc-modified-id=\"DEALING-WITH-CLASS-IMBALANCE-4.4\">DEALING WITH CLASS IMBALANCE</a></span></li><li><span><a href=\"#Checking-for-Outliers\" data-toc-modified-id=\"Checking-for-Outliers-4.5\">Checking for Outliers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Checking-for-outliers-using-DBSCAN.\" data-toc-modified-id=\"Checking-for-outliers-using-DBSCAN.-4.5.1\">Checking for outliers using DBSCAN.</a></span></li></ul></li></ul></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-5\">Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#A.-Interactions-among-Features\" data-toc-modified-id=\"A.-Interactions-among-Features-5.1\">A. Interactions among Features</a></span></li><li><span><a href=\"#B.-Dimensionality-reduction-using-PCA\" data-toc-modified-id=\"B.-Dimensionality-reduction-using-PCA-5.2\">B. Dimensionality reduction using PCA</a></span></li></ul></li><li><span><a href=\"#Feature-Selection-and-Model-Building\" data-toc-modified-id=\"Feature-Selection-and-Model-Building-6\">Feature Selection and Model Building</a></span></li><li><span><a href=\"#SUMMARY\" data-toc-modified-id=\"SUMMARY-7\">SUMMARY</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION\n",
    "In my previous research, I trained a **Logistic Regression** model with data that I hadn't extensively preprocessed. I did not follow any of the assumptions with regards to using Logistic Regression, and I was able to achieve an accuracy of about 80%. In this notebook however I will follow the assumptions, and observe how my model performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEDUCTIONS FROM THE PREVIOUS RESEARCH.\n",
    "- With k-fold cross validation, I only achieved an accuracy of 80% on the test set.\n",
    "- With the learning curve, I was able to tell that adding more training samples won't do any good.\n",
    "- With the learning curve, I was also able to tell that the model totally \"underfits\" the data.\n",
    "- With the validation curve, I was also able to tell that our model didn't do too well with variations in regularization strength.\n",
    "- We noticed that there was large imbalance in our classes with about an 80:20 ratio.\n",
    "- With the confusion matrix, I was able to tell the number of misclassified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSUMPTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logisitic regression does not **require a linear relationship** between the dependent and independent variable.\n",
    "- **Homoscedasticity** is not required unlike in linear regression.\n",
    "- The dependent variable is **not** measured on an interval.\n",
    "- Observations must be **independent** of each other, in other words, observations should not come from repeated measurements or matched data.\n",
    "- Logistic regression assumes **linearity of independent variables and log odds**.\n",
    "- The dependent variable should be **dichotomous** in nature.\n",
    "- It works well when there are **no outliers** in your data.\n",
    "- It also works well when there is **no multicollinearity** in our predictors. \n",
    "- The dataset must be **linearly seperable**.\n",
    "- Logistic regression works well with a **balance in class distribution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TACKLING ASSUMPTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating Relationships between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "## grabbing the data\n",
    "data = pd.read_csv('Churn_Modelling.csv')\n",
    "\n",
    "## dropping 3 features\n",
    "data = data.drop(['RowNumber', 'CustomerId', 'Surname', 'Exited'], axis=1)\n",
    "\n",
    "## encoding categorical features\n",
    "geo_dict = {val:idx for idx, val in enumerate(np.unique(data['Geography'].values))}\n",
    "gen_dict = { val : idx for idx, val in enumerate(np.unique(data['Gender'].values))}\n",
    "\n",
    "## mapping \n",
    "data['Geography'] = data['Geography'].map(geo_dict)\n",
    "data['Gender'] = data['Gender'].map(gen_dict)\n",
    "\n",
    "## this is where seaborn's pairplot() function shines\n",
    "\n",
    "##sns.pairplot(data)\n",
    "data.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell from our plot that some of the features are linearly separable where as some are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "cols = data.columns\n",
    "cm = np.corrcoef(data[cols].values.T)\n",
    "sns.set(font_scale=1.5)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size' : 15}, yticklabels=cols, xticklabels=cols)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the strength of collinearity is indeed very weak with the highest being 0.3 (Balance and NumOfProducts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's count the number of customers who have churned as well as those who haven't\n",
    "data = pd.read_csv('Churn_Modelling.csv')\n",
    "exited = np.array(data['Exited'].values)\n",
    "churned = np.sum(exited == 1).astype(int)\n",
    "not_churned = np.sum(exited == 0).astype(int)\n",
    "total = churned + not_churned\n",
    "print('The percentage of customers that churned: %.3f%%' % (churned/total * 100))\n",
    "print('The percentage of customers that did not churned: {}%'.format(not_churned/total * 100))\n",
    "print('the total number of samples: %d' % int(total))\n",
    "\n",
    "\"\"\"\n",
    "or \n",
    "df = pd.read_csv('Churn_Modelling.csv')\n",
    "df['Exited'].value_counts().sort_values(ascending=False)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEALING WITH CLASS IMBALANCE\n",
    "Constructing a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "## grab the entire data\n",
    "data = pd.read_csv('Churn_Modelling.csv')\n",
    "\n",
    "## drop a few columns\n",
    "data_ = data.drop(['RowNumber', 'CustomerId', 'Surname', 'Exited'], axis=1)\n",
    "\n",
    "## grab the target variable\n",
    "exited = data['Exited']\n",
    "\n",
    "## grab the unique values\n",
    "values = data['Exited'].value_counts()\n",
    "print ('Number of class 1 samples before : {}'.format(values[1]))\n",
    "\n",
    "## let's grab our training data\n",
    "data_upsampled, exited_upsampled = resample(data_[exited == 1], exited[exited == 1], replace=True, n_samples=data_[exited == 0].shape[0], random_state=123)\n",
    "\n",
    "## Now let's find out the number of class 1 samples\n",
    "print('Number of class 1 samples after : {}'.format(data_upsampled.shape[0]))\n",
    "\n",
    "## let's now stack the original class 0 samples with the upsampled class 1 subset.\n",
    "data_bal = np.vstack((data_[exited == 0], data_upsampled))\n",
    "exited_bal = np.hstack((exited[exited == 0], exited_upsampled))\n",
    "\n",
    "# majority vote prediction should achieve 50 percent\n",
    "# y_pred = np.zeros(exited_bal.shape[0])\n",
    "# np.mean(y_pred == exited_bal)\n",
    "\n",
    "\n",
    "data_bal = pd.DataFrame(data_bal, columns=['Credit Score', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary'])\n",
    "\n",
    "## let's dummy up a few features\n",
    "geog_gend = pd.get_dummies(data_bal[['Geography', 'Gender']], drop_first=True)\n",
    "\n",
    "## appending the dummied features to the main dataframe\n",
    "## first drop the original data\n",
    "data_bal = data_bal.drop(['Geography', 'Gender'], axis=1)\n",
    "\n",
    "## now append the dummied features\n",
    "data_bal['Geography_Germany'] = geog_gend['Geography_Germany']\n",
    "data_bal['Geography_Spain'] = geog_gend['Geography_Spain']\n",
    "data_bal['Gender_Male'] = geog_gend['Gender_Male']\n",
    "\n",
    "# let's append 'exited' to this dataframe for future use\n",
    "data_bal['Exited'] = exited_bal\n",
    "\n",
    "## saving the data_bal\n",
    "data_bal.to_csv('data_bal.csv', index=False)\n",
    "\n",
    "## reading data_bal back in\n",
    "data_bal = pd.read_csv('data_bal.csv')\n",
    "data_bal.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe I imported earlier ***(data)*** has been modified now ***(data_bal)***. I'll use this dataframe for further analysis ***(data_bal)***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a huge imbalance in the distribution of our class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various ways to test for outliers in our data however in this notebook, I'll explore **1 multivariate method** and **1 univariate method** - **DBSCAN (Density Clustering)** and the **Tukey Method using a box plot** respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data_bal = pd.read_csv('data_bal.csv')\n",
    "\n",
    "features = ['EstimatedSalary', 'Age', 'Balance', 'Credit Score', 'Tenure', 'NumOfProducts']\n",
    "\n",
    "green_diamond = dict(markerfacecolor='g', marker='o')\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, len(features), figsize=(30, 5))\n",
    "for feature in features:\n",
    "        axes[features.index(feature)].boxplot(data_bal[feature], flierprops=green_diamond)\n",
    "        axes[features.index(feature)].set_title('Box Plot for %s' % feature, loc='center', pad=5 )\n",
    "        plt.tight_layout()\n",
    "        \n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have the most Outliers in the **Age** feature, with the **Credit Scores** to follow. The **Number of Products feature** has just an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for outliers using DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBSCAN - Density Based Spatial Clustering of Applications with Noise**, assigns cluster labels based on **dense regions of points**. In DBSCAN, the notion of density is density is defined as the number of points within a specified radius, **r**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_bal = pd.read_csv('data_bal.csv')\n",
    "\n",
    "X = np.array(data_bal[['Age', 'Balance']])\n",
    "sc = StandardScaler()\n",
    "X_std = sc.fit_transform(X)\n",
    "# plt.scatter(X_std[:900, 0], X_std[:900, 1])\n",
    "db = DBSCAN(eps=0.4, min_samples=20, metric='euclidean')\n",
    "y_db = db.fit_predict(X_std)\n",
    "plt.scatter(X_std[y_db == -1, 0], X_std[y_db == -1, 1], c='lightblue', marker='o', s=40, label='cluster 1')\n",
    "plt.scatter(X_std[y_db == 0, 0], X_std[y_db == 0, 1], c='red', marker='o', s=40, label='cluster 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "np.unique(y_db)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is super inaccurate and i am hoping that someone can help me with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, increasing or decreasing dimensionality. \n",
    "### A. Interactions among Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A simple two-way interaction is represented by:\n",
    "     - X3 = X1 * X2 where X3 is the interaction between X1 and X2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding interaction terms as additional new features to your model is useful for your model if the impact of two or more features on the outcome is **non-additive**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that interactions amongst dummy variables belonging to the same categorical feature are always zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is very easy to calculate two-way interactions amongst all features, it is very computationally expensive\n",
    "- 10 features = 45 two-way interaction terms\n",
    "- 50 features = 1,225 two-way interaction terms\n",
    "- 100 features = 4,950 two-way interaction terms\n",
    "- 500 features = 124,750 two-way interaction terms\n",
    "- Recommend understanding your data and domain if possible and selectively choosing   interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use PolynomialFeatures in sklearn.preprocessing to creat two-way interactions for all features\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def add_interactions(df):\n",
    "    # Get feature names\n",
    "    combos = list(combinations(list(df.columns), 2))\n",
    "    colnames = list(df.columns) + [' '.join(x) for x in combos]\n",
    "    \n",
    "    # Find interactions\n",
    "    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "    df = poly.fit_transform(df)\n",
    "    df = pd.DataFrame(df)\n",
    "    df.columns = colnames\n",
    "    \n",
    "    ## Remove interaction terms with all 0 values\n",
    "    noint_indices = [i for i, x in enumerate(list((df == 0).all())) if x]\n",
    "    df = df.drop(df.columns[noint_indices], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_bal = pd.read_csv('data_bal.csv')\n",
    "new_data_bal = add_interactions(data_bal.drop(['Exited'], axis=1))\n",
    "new_data_bal.to_csv('data_balAddedInteractions.csv', index=False)\n",
    "\n",
    "## reading it back in\n",
    "new_data_bal = pd.read_csv('data_balAddedInteractions.csv')\n",
    "new_data_bal.head()\n",
    "\n",
    "## so new_data_bal doesn't have the 'Exited' feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Dimensionality reduction using PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Principal component analysis (PCA) is a technique that transforms a dataset of many features into principal components that \"summarize\" the variance that underlies the data.\n",
    "\n",
    "\n",
    "- Each principle component is calculated by finding the linear combination of features that maximizes variance, while also ensuring zero correlation with the previously calculated princpal components.\n",
    "\n",
    "\n",
    "- Use cases for modelling:\n",
    "    - One of the most common dimensionality reduction techniques.\n",
    "    - Use if there are too many features or if observation/feature ratio is poor.\n",
    "    - Also, potentially good option if there are a lot of highly correlated variables in your dataset.\n",
    "    \n",
    "    \n",
    "- Unfortunately, PCA makes models a lot harder to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50)\n",
    "data_pc_pca = pd.DataFrame(pca.fit_transform(data_bal))\n",
    "data_pc_pca.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will not reduce the dimensions of the dataset now though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pc_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Feature Selection and Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll build our model using the data we've processed. Keeping our fingers crossed ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_bal = pd.read_csv('data_bal.csv')\n",
    "new_data_bal = pd.read_csv('data_balAddedInteractions.csv')\n",
    "exited = data_bal['Exited']\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_data_bal, exited, random_state=1, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_selection\n",
    "\n",
    "## select the number of best features\n",
    "select = sklearn.feature_selection.SelectKBest(k=50)\n",
    "\n",
    "## get the selected features\n",
    "selected_features = select.fit(X_train, y_train)\n",
    "\n",
    "## get the indices of the selected features\n",
    "indices_selected = selected_features.get_support(indices=True)\n",
    "\n",
    "## get the names of the features\n",
    "colnames_selected = [new_data_bal.columns[i] for i in indices_selected]\n",
    "\n",
    "\n",
    "## get the samples for the selected features\n",
    "X_train_selected = X_train[colnames_selected]\n",
    "X_test_selected = X_test[colnames_selected]\n",
    "\n",
    "\n",
    "## standardizing the samples\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train_selected)\n",
    "X_test_std = sc.transform(X_test_selected)\n",
    "\n",
    "\n",
    "## training and evaluating the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# the model\n",
    "## Using a **class_weight** of balanced assigns a larger penalty to wrong predictions on the minority class. \n",
    "## This is one way of dealing with ***CLASS IMBALANCE***.\n",
    "model = LogisticRegression(random_state=1, solver='lbfgs', max_iter = 1000)\n",
    "model.fit(X_train_std, y_train)\n",
    "\n",
    "## get probabilities\n",
    "y_hat = [x[1] for x in model.predict_proba(X_test_std)]\n",
    "\n",
    "## finding the area under the curve\n",
    "auc = roc_auc_score(y_test, y_hat)\n",
    "\n",
    "## finding the accuracy\n",
    "score_test = model.score(X_test_std, y_test)\n",
    "score_train = model.score(X_train_std, y_train)\n",
    "\n",
    "## TODO: Further model evaluation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Tree of Contents",
   "title_sidebar": "Tree of Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
